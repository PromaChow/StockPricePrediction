name: Trigger Airflow DAG Workflow

on:
  push:
    branches:
      - main
      - citest
  workflow_dispatch:

jobs:
  trigger_airflow_dag:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose
      - name: Initialize Airflow
        working-directory: ./pipeline/airflow
        run: |
          docker-compose up airflow-init
      - name: Start Airflow Services
        working-directory: ./pipeline/airflow
        run: |
          docker-compose up -d
      # # Step 6: Install Python packages inside Docker containers
      # - name: Install Python Packages
      #   working-directory: ./pipeline/airflow
      #   run: |
      #     docker-compose exec -T airflow-scheduler python3 -m pip install -r /opt/airflow/dags/requirements.txt
      #     # docker-compose exec -T airflow-webserver python3 -m pip install -r /opt/airflow/dags/requirements.txt


      # Step 6: Set permissions for Airflow logs inside the container
      - name: Set permissions for Airflow logs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T --user root airflow-scheduler bash -c "chmod -R 777 /opt/airflow/logs/"
      - name: Wait for Airflow to Initialize
        working-directory: ./pipeline/airflow
        run: |
          timeout 300 bash -c 'until docker-compose exec -T airflow-webserver curl -f http://localhost:8080/health; do sleep 10; done'
  
      # Step 9: Delete .pyc Files
      - name: Delete .pyc Files
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler find /opt/airflow -name \*.pyc -delete
          docker-compose exec -T airflow-webserver find /opt/airflow -name \*.pyc -delete
      - name: List DAG Import Errors
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags list-import-errors
      - name: Show Airflow DAGs
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags list
      - name: Trigger Airflow DAG
        working-directory: ./pipeline/airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags trigger -r manual_$(date +%Y%m%d%H%M%S) Group10_DataPipeline_MLOps
      - name: Monitor DAG Execution
        working-directory: ./pipeline/airflow
        run: |
          for i in {1..10}; do
            STATUS=$(docker-compose exec -T airflow-scheduler airflow dags state Group10_DataPipeline_MLOps $(date +%Y-%m-%d))
            echo "Current DAG status: $STATUS"
            if [ "$STATUS" = "success" ]; then
              echo "DAG completed successfully"
              break
            elif [ "$STATUS" = "failed" ]; then
               echo "DAG failed"
              exit 1
            fi
            sleep 60
          done
      - name: Stop Airflow Services
        if: always()
        working-directory: ./pipeline/airflow
        run: docker-compose down --volumes --rmi all